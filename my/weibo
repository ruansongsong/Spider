#coding:utf-8
import urllib
import urllib2
import re
import os
import time
import random
import cookielib
from bs4 import BeautifulSoup
import sys
reload(sys)
sys.setdefaultencoding('utf8')
def get_html(url):
	'''
	mycookie = urllib2.HTTPCookieProcessor(cookielib.CookieJar())
	openner = urllib2.build_opener(mycookie)
	request = urllib2.Request(url)
	request.add_header("Accept-Language", "zh-CN,zh;q=0.8")
	request.add_header("Connection", "keep-alive")
	request.add_header("Accept-Encoding", "gzip, deflate, sdch")
	request.add_header("X-Requested-With", "XMLHttpRequest")
	request.add_header("Referer", url)
	request.add_header("User-Agent","Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.130 Safari/537.36")
	cookie_str = "*SINAGLOBAL=4121053374838.084.1439131958339; _s_tentry=blog.csdn.net; Apache=7963410776574.165.1440491828008; ULV=1440491828263:6:6:1:7963410776574.165.1440491828008:1439953133203; login_sid_t=ce913e4155215ed276ae84884eb6a766; WBtopGlobal_register_version=a3f5184be4b5f58b; un=18813298638; appkey=; myuid=5686977920; un=ruansongsong@sohu.com; wvr=6; SWB=usrmdinst_20; WBStore=4e40f953589b7b00|undefined; SUS=SID-2881082742-1440762532-GZ-qaxmb-a6943026de8132f8bee48edd6e345a23; SUE=es%3Dc885b3531f3f136c545faa110eca6a2a%26ev%3Dv1%26es2%3D08e7bb289cac66e9d1c5df8e0ef1e25a%26rs0%3DVu011%252BwhKohLceqLqtWxKOgn3gWm3qSGyOnb6MbN%252FKSHU%252BmDkzExqrZ73cVzgR8X7RTgO9%252FZ79GXK9x%252Fjji1fDZlPlwUq2blICavdpOwCsHKPMTRHui1c8Mr%252B%252B19UgLBK7vjL%252BKedeB8qAn12zlYMjJIzro6TpVWw%252F5SIooK8Pw%253D%26rv%3D0; SUP=cv%3D1%26bt%3D1440762532%26et%3D1440848932%26d%3Dc909%26i%3D5a23%26us%3D1%26vf%3D0%26vt%3D0%26ac%3D19%26st%3D0%26uid%3D2881082742%26name%3Druansongsong%2540sohu.com%26nick%3D%25E7%2594%25A8%25E6%2588%25B72881082742%26fmp%3D%26lcp%3D; SUB=_2A2545Dr0DeTxGeRG41MR-CzLzz6IHXVbkCs8rDV8PUNbvtBeLUzWkW-igZyEpGQV5nBOkxzKC8a0fiXDrQ..; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W550s9CMhWrSiZond.UcQBw5JpX5KMt; SUHB=0Ry6bwsm8X_Znp; ALF=1472298531; SSOLoginState=1440762532; UOR=www.17sucai.com,widget.weibo.com,qinxuye.me"
	request.add_header("Cookie", cookie_str)
	html = openner.open(request).read()
	'''
	#print html_src
	headers={
		"Host":"s.weibo.com",
		"X-Requested-With":"XMLHttpRequest",
		"User-Agent":"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.125 Safari/537.36",
		"Cookie":"SINAGLOBAL=4121053374838.084.1439131958339; _s_tentry=blog.csdn.net; Apache=7963410776574.165.1440491828008; ULV=1440491828263:6:6:1:7963410776574.165.1440491828008:1439953133203; login_sid_t=ce913e4155215ed276ae84884eb6a766; WBtopGlobal_register_version=a3f5184be4b5f58b; un=18813298638; appkey=; myuid=5686977920; un=ruansongsong@sohu.com; wvr=6; SWB=usrmdinst_20; WBStore=4e40f953589b7b00|undefined; SUS=SID-2881082742-1440762532-GZ-qaxmb-a6943026de8132f8bee48edd6e345a23; SUE=es%3Dc885b3531f3f136c545faa110eca6a2a%26ev%3Dv1%26es2%3D08e7bb289cac66e9d1c5df8e0ef1e25a%26rs0%3DVu011%252BwhKohLceqLqtWxKOgn3gWm3qSGyOnb6MbN%252FKSHU%252BmDkzExqrZ73cVzgR8X7RTgO9%252FZ79GXK9x%252Fjji1fDZlPlwUq2blICavdpOwCsHKPMTRHui1c8Mr%252B%252B19UgLBK7vjL%252BKedeB8qAn12zlYMjJIzro6TpVWw%252F5SIooK8Pw%253D%26rv%3D0; SUP=cv%3D1%26bt%3D1440762532%26et%3D1440848932%26d%3Dc909%26i%3D5a23%26us%3D1%26vf%3D0%26vt%3D0%26ac%3D19%26st%3D0%26uid%3D2881082742%26name%3Druansongsong%2540sohu.com%26nick%3D%25E7%2594%25A8%25E6%2588%25B72881082742%26fmp%3D%26lcp%3D; SUB=_2A2545Dr0DeTxGeRG41MR-CzLzz6IHXVbkCs8rDV8PUNbvtBeLUzWkW-igZyEpGQV5nBOkxzKC8a0fiXDrQ..; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W550s9CMhWrSiZond.UcQBw5JpX5KMt; SUHB=0Ry6bwsm8X_Znp; ALF=1472298531; SSOLoginState=1440762532; UOR=www.17sucai.com,widget.weibo.com,qinxuye.me"
	}
	mycookie = urllib2.HTTPCookieProcessor(cookielib.CookieJar())
	openner = urllib2.build_opener(mycookie)
	#request = urllib2.Request(url)
	req=urllib2.Request(url)
	for keys in headers:
		req.add_header(keys,headers[keys])
	html = openner.open(req).read()
	#html=urllib2.urlopen(req).read()
	lines=html.splitlines()
	for line in lines:
		if line.startswith('<script>STK && STK.pageletM && STK.pageletM.view({"pid":"pl_weibo_direct'):
			#print "not caught"
			n = line.find('"html":"')
			if n > 0:
				get_html = line[n + 8: ].encode("utf-8").decode('unicode_escape').encode("utf-8").replace("\\", "")
			#else:
			#	print "caught"
	#html=html1.encode("utf-8").decode('unicode_escape').encode("utf-8").replace("\\", "")
	return get_html
'''
  lines=text.splitlines()    
        for line in lines:  
         if line.startswith('<script>STK && STK.pageletM && STK.pageletM.view({"pid":"pl_relation_myf'):  
            n = line.find('html":"')    
            if n > 0:    
                j = line[n + 7: -12].replace("\\", "")    
                soup =BeautifulSoup(j)  
                follows=soup.findAll('div','myfollow_list S_line2 SW_fun')  
'''
'''
def get_nicknamelist(html):
	reg=r'(?<=feed_list_content\" nick-name=\").+?(?=\")'
	nickname_re=re.compile(reg)
	nickname_list=re.findall(nickname_re,html)
	return nickname_list
'''
def get_details(html):
	soup=BeautifulSoup(html)
	#得到作者、作者链接、微博正文
	div_content=soup.find_all(attrs={'class': 'content clearfix'})
	#得到发微博时间
	div_time=soup.find_all(attrs={'class':'feed_from W_textb'})
	nick_name=[]
	nickname_href=[]
	content_text=[]
	time=[]
	#print get_content[0]
	for i in range(len(div_content)):
		#查找a标签
		a_tag=div_content[i].find('a')
		nick_name.append(a_tag.get('nick-name'))
		nickname_href.append(a_tag.get('href'))
		#查找p标签
		p_tag=div_content[i].find('p')
		content_text.append(p_tag.get_text())
	#print get2.get_text()
	#print len(get2)
	#得到发微博时间
	for j in range(len(div_time)):
		a_time=div_time[j].find('a')
		time.append(a_time.get('title'))
	return (nick_name,nickname_href,content_text,time)
def get_number_info(html):#一次性得到所有数字
	soup = BeautifulSoup(html)
	#print soup.p.get('name')
	#获取标签内容
	#print soup.p.string
	#查找
	get=soup.find_all(attrs={'class': 'feed_action_info feed_action_row4'})
	#print len(get)
	#print get[1]
	get_number_info=[]#收集转发、评论、赞的数据
	for i in range(len(get)):
		#转发
		#print get[0]
		#break
		#action-type="feed_list_forward"
		forward=get[i].find(attrs={'action-type':'feed_list_forward'})
		#print forward
		#print len(forward)
		#a=forward[0].find_all('em')
		#b=a[0]
		#print type(b)
		forward_em=forward.find_all('em')
		#print forward_em[0].get_text()
		#print forward_em.get_text()
		if (len(forward_em[0].get_text())==0):
			temp_forward="0"
			get_number_info.append(temp_forward)
		else:
			temp_forward=forward_em[0].get_text()
			get_number_info.append(temp_forward)
		#评论
		comment=get[i].find(attrs={'action-type':'feed_list_comment'})
		if bool(comment.find_all('em')):
			comment_em=comment.find_all('em')
			temp_comment=comment_em[0].get_text()
			get_number_info.append(temp_comment)
		else:
			temp_comment="0"
			get_number_info.append(temp_comment)
		#赞
		like=get[i].find(attrs={'action-type':'feed_list_like'})
		like_em=like.find_all('em')
		if (len(like_em[0].get_text())==0):
			temp_like="0"
			get_number_info.append(temp_like)
		else:
			temp_like=like_em[0].get_text()
			get_number_info.append(temp_like)
	return get_number_info
def write_all_info(nick_name,nickname_href,content_text,times,number_info):
	nick_name_list=nick_name
	nickname_href_list=nickname_href
	content_text_list=content_text
	time_list=times
	number_info_list=number_info
	path='E:/ruansongsong/'
	isExists=os.path.exists(path)
	if not isExists:
	    os.makedirs(path)
	temp=0
	for i in range(len(nick_name)):
		write_all_list=open(path+"weibo.txt",'a')
		write_all_list.writelines("微博用户名称："+nick_name_list[i]+"\n"+"微博链接："+nickname_href[i]+"\n"+"正文:"+"\n"+content_text_list[i]+"\n"+"发微博时间："+time_list[i]+"\n")
		j=0
		#write_all_list.writelines("转发："+"  "+"评论："+"   "+"赞："+"   "+"\n")
		while (j!=3):
			write_all_list.writelines("==="+number_info_list[temp]+"==="+"\n")
			j+=1
			temp+=1
		write_all_list.close()

#keyword="天津"
#爬取多页，应该还是被识别成爬虫了
'''
url="http://s.weibo.com/weibo/%25E6%2596%25B0iPhone&page=8"
print url
html=get_html(url)
number_info=get_number_info(html)
(nick_name,nickname_href,content_text,times)=get_details(html)
write_all_info(nick_name,nickname_href,content_text,times,number_info)
print len(nick_name)
print nick_name
print nickname_href
print content_text
'''
i=1
while (i<60):
	url="http://s.weibo.com/weibo/%25E5%25A5%2587%25E8%2591%25A9%25E8%25AF%25B4&page="+str(i)
	print url
	html=get_html(url)
	number_info=get_number_info(html)
	print len(number_info)
	(nick_name,nickname_href,content_text,times)=get_details(html)
	write_all_info(nick_name,nickname_href,content_text,times,number_info)
	sleeptime_rand = random.randint(1,30)
	print sleeptime_rand
	time.sleep(sleeptime_rand)
	i+=1
#url="http://s.weibo.com/weibo/%25E5%25A4%25A9%25E6%25B4%25A5&region=custom:11:1000&typeall=1&suball=1&timescope=custom:2015-08-01-0:2015-08-20-0&Refer=g"
#url_quote=urllib.quote(url)

#print len(number_info)
#print number_info

#print len(nick_name)
#write_all_info(nick_name,nickname_href,content_text,time,number_info)
#print nick_name
#print nickname_href
#print content_text
#print time

'''
get=get_nicknamelist(html)
for i in range(len(get)):
	print get[i]+"\n"
'''